{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Sentiment variations using LLMs\n",
    "\n",
    "Author: Raphael Merx \\\\\n",
    "Input: a baseline sentence; Output: variations of this sentence where a target word has more positive or negative sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simplemind python-dotenv pandas openpyxl\n",
    "#!pip install openpyxl==3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import simplemind as sm\n",
    "from typing import Literal, List, get_args\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Define TARGET_WORD_CHOICES\n",
    "TARGET_WORD_CHOICES = Literal['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "TARGET_WORD: TARGET_WORD_CHOICES = 'abuse'\n",
    "\n",
    "# Generate human-readable versions for all target words\n",
    "TARGET_WORD_HUMAN_CHOICES = {word: word.replace('_', ' ') for word in get_args(TARGET_WORD_CHOICES)}\n",
    "TARGET_WORD_HUMAN = TARGET_WORD_HUMAN_CHOICES[TARGET_WORD]\n",
    "\n",
    "# Validate that both 'mental_health' and 'mental_illness' are handled correctly\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_health'] == 'mental health', \"Error: 'mental_health' not handled correctly\"\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_illness'] == 'mental illness', \"Error: 'mental_illness' not handled correctly\"\n",
    "\n",
    "# 1970-1974, ..., 2015-2019\n",
    "EPOCH_CHOICES = [f\"{y}-{y+4}\" for y in range(1970, 2020, 5)]\n",
    "EPOCH = EPOCH_CHOICES[0]\n",
    "\n",
    "MAX_BASELINES = 1500\n",
    "\n",
    "# can be changed to gemini, see https://pypi.org/project/simplemind/\n",
    "PROVIDER = \"openai\"\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get neutral baseline sentences from corpus for LLM input for each target\n",
    "\n",
    "- **Script 1 Aim**: This script computes sentence-level sentiment scores using the NRC-VAD lexicon for a corpus of target terms spanning 1970–2019. It dynamically determines neutral sentiment ranges for each 5-year epoch by expanding outward from the median sentiment score within the interquartile range (Q1–Q3) until at least 500 sentences are included, capped at 1500. The selected sentences are saved to CSV files for each target and epoch, and a summary file logs the dynamic ranges and counts.\n",
    "\n",
    "- **Script 2 Aim**: This script processes pre-saved baseline CSV files to calculate sentence counts by year and 5-year epochs for multiple target terms. It generates \"year_count_lines.csv\" and \"epoch_count_lines.csv\" summarizing these counts and creates an epoch-based bar plot visualizing sentence distributions across the specified epochs for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\abuse_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\anxiety_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\depression_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_health_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\mental_illness_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\trauma_2015-2019.baseline_1500_sentences.csv\n",
      "Year count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\output\\year_count_lines.csv.\n",
      "Epoch count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\1_sentiment\\synthetic\\input\\baselines\\output\\epoch_count_lines.csv.\n",
      "Epoch plot saved to ../../figures\\plot_appendixB_sentiment.png.\n"
     ]
    }
   ],
   "source": [
    "#%run step0_get_neutral_baselines_sentiment.py\n",
    "#%run step1_plot_neutral_baselines_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup examples to inject in the prompt\n",
    "\n",
    "-This code sets up examples of baseline sentences and their sentiment-modified variations (more and less intense) to provide context and guidance for the LLM. \n",
    "-These examples are formatted into a structured prompt to help the model understand how to generate sentiment-modified variations for new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    baseline: str\n",
    "    positive_sentiment: str\n",
    "    negative_sentiment: str\n",
    "\n",
    "    def format_for_prompt(self, target_word: TARGET_WORD_CHOICES):\n",
    "        return f\"\"\"<baseline>\n",
    "{self.baseline}\n",
    "</baseline>\n",
    "<positive {target_word}>\n",
    "{self.positive_sentiment}\n",
    "</positive {target_word}>\n",
    "<negative {target_word}>\n",
    "{self.negative_sentiment}\n",
    "</negative {target_word}>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def read_example_data(word: TARGET_WORD_CHOICES) -> pd.DataFrame:\n",
    "        # find the sentiment_example_sentences.xlsx file in the `input` folder\n",
    "        filepath = os.path.join('input', f'sentiment_example_sentences.xlsx')\n",
    "        df = pd.read_excel(filepath)\n",
    "        df = df[df['target'] == word]\n",
    "        return df\n",
    "\n",
    "def get_examples(target_word: TARGET_WORD_CHOICES) -> List[Example]:\n",
    "    example_data = Example.read_example_data(target_word)\n",
    "\n",
    "    return [\n",
    "        Example(\n",
    "            baseline=row['baseline'],\n",
    "            positive_sentiment=row['positive_sentiment'],\n",
    "            negative_sentiment=row['negative_sentiment']\n",
    "        )\n",
    "        for _, row in example_data.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "PROMPT_INTRO = \"\"\"In psychology research, 'Sentiment' is defined as “a term’s acquisition of a more positive or negative connotation.” This task focuses on the sentiment of the term **<<{target_word}>>**.\n",
    "\n",
    "### **Task**  \n",
    "You will be given a sentence containing the term **<<{target_word}>>**. Your goal is to write two new sentences:\n",
    "1. One where **<<{target_word}>>** has a **more positive connotation** (enclose this sentence between `<positive {target_word}>` and `</positive {target_word}>` tags).  \n",
    "2. One where **<<{target_word}>>** has a **more negative connotation** (enclose this sentence between `<negative {target_word}>` and `</negative {target_word}>` tags).\n",
    "\n",
    "### **Rules**  \n",
    "1. The term **<<{target_word}>>** must remain **exactly as it appears** in the original sentence:\n",
    "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
    "   - Synonyms, variations, or altered spellings are not allowed.  \n",
    "\n",
    "2. **Meaning and Structure**:  \n",
    "   - Stay true to the original context and subject matter.  \n",
    "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
    "\n",
    "3. **Sentiment Adjustments**:  \n",
    "   - **Positive Sentiment**: Reflect strengths or benefits realistically, while respecting the potential negativity of **<<{target_word}>>**.\n",
    "   - **Negative Sentiment**: Highlight risks or harms appropriately, avoiding exaggeration or trivialization.  \n",
    "\n",
    "### **Important**  \n",
    "- Any response omitting, replacing, or altering **<<{target_word}>>** will be rejected.  \n",
    "- Ensure the output is:  \n",
    "   - **Grammatically correct**  \n",
    "   - **Sensitive and serious** in tone  \n",
    "   - **Free from exaggeration or sensationalism**  \n",
    "   - **Strictly following the XML-like tag format for sentiment variations**\n",
    "\n",
    "Follow these guidelines strictly to produce valid responses.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word trauma, epoch 1970-1974: Loaded 13 baseline sentences, sampling 1500\n",
      "In psychology research, 'Sentiment' is defined as “a term’s acquisition of a more positive or negative connotation.” This task focuses on the sentiment of the term **<<trauma>>**.\n",
      "\n",
      "### **Task**  \n",
      "You will be given a sentence containing the term **<<trauma>>**. Your goal is to write two new sentences:\n",
      "1. One where **<<trauma>>** has a **more positive connotation** (enclose this sentence between `<positive trauma>` and `</positive trauma>` tags).  \n",
      "2. One where **<<trauma>>** has a **more negative connotation** (enclose this sentence between `<negative trauma>` and `</negative trauma>` tags).\n",
      "\n",
      "### **Rules**  \n",
      "1. The term **<<trauma>>** must remain **exactly as it appears** in the original sentence:\n",
      "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
      "   - Synonyms, variations, or altered spellings are not allowed.  \n",
      "\n",
      "2. **Meaning and Structure**:  \n",
      "   - Stay true to the original context and subject matter.  \n",
      "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
      "\n",
      "3. **Sentiment Adjustments**:  \n",
      "   - **Positive Sentiment**: Reflect strengths or benefits realistically, while respecting the potential negativity of **<<trauma>>**.\n",
      "   - **Negative Sentiment**: Highlight risks or harms appropriately, avoiding exaggeration or trivialization.  \n",
      "\n",
      "### **Important**  \n",
      "- Any response omitting, replacing, or altering **<<trauma>>** will be rejected.  \n",
      "- Ensure the output is:  \n",
      "   - **Grammatically correct**  \n",
      "   - **Sensitive and serious** in tone  \n",
      "   - **Free from exaggeration or sensationalism**  \n",
      "   - **Strictly following the XML-like tag format for sentiment variations**\n",
      "\n",
      "Follow these guidelines strictly to produce valid responses.  \n",
      "\n",
      "\n",
      "<baseline>\n",
      "This paper presents a cognitive-behavioral model for conceptualizing and intervening in the area of sexual trauma.\n",
      "</baseline>\n",
      "<positive trauma>\n",
      "This paper celebrates a cognitive-behavioral model for promoting new ideas and helping in the area of sexual trauma.\n",
      "</positive trauma>\n",
      "<negative trauma>\n",
      "This paper presents a cognitive-behavioral model for thinking about and wresting with the harmful problem of sexual trauma.\n",
      "</negative trauma>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "In most classrooms in most schools, there are students who have suffered complex trauma who would benefit from a system-wide, trauma-informed approach to schooling.\n",
      "</baseline>\n",
      "<positive trauma>\n",
      "In most classrooms in most schools, there are students who have experienced complex trauma who would benefit from a system-wide, responsive and enlightened approach to schooling.\n",
      "</positive trauma>\n",
      "<negative trauma>\n",
      "In most classrooms in most schools, there are students who have suffered damaging trauma whose problems need a system-wide, illness-based approach to schooling.\n",
      "</negative trauma>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Research has shown that women are more likely to develop PTSD subsequent to trauma exposure in comparison with men.\n",
      "</baseline>\n",
      "<positive trauma>\n",
      "Research has shown that women are more likely to develop PTSD subsequent to trauma experiences in comparison with men.\n",
      "</positive trauma>\n",
      "<negative trauma>\n",
      "Research has shown that women are more likely to deteriorate into PTSD subsequent to trauma exposure in comparison with men.\n",
      "</negative trauma>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Numerous homeless youth experience trauma prior to leaving home and while on the street.\n",
      "</baseline>\n",
      "<positive trauma>\n",
      "Numerous resilient youth learn to navigate trauma prior to leaving home and while adapting to life on the street.\n",
      "</positive trauma>\n",
      "<negative trauma>\n",
      "Numerous homeless youth endure significant trauma prior to leaving home and while facing severe challenges on the street.\n",
      "</negative trauma>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The meaning of trauma within psychology has for a long time been viewed mostly from a pathologizing standpoint.\n",
      "</baseline>\n",
      "<positive trauma>\n",
      "The meaning of trauma within psychology has for a long time needed to be viewed from a more compassionate and strengths-based standpoint.\n",
      "</positive trauma>\n",
      "<negative trauma>\n",
      "The meaning of trauma within psychology has for a long time been viewed mostly from a negative and overly disease-focused standpoint.\n",
      "</negative trauma>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The present paper describes a simple and accurate technique for the artificial respiration of curarized rats that avoids the trauma involved in a tracheotomy.\n",
      "</baseline>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SentenceToModify:\n",
    "    text: str\n",
    "    positive_variation: str = None\n",
    "    negative_variation: str = None\n",
    "\n",
    "    def get_prompt(self, target_word: str) -> str:\n",
    "        prompt = PROMPT_INTRO.format(target_word=target_word) + \"\\n\\n\"\n",
    "        for example in get_examples(target_word):\n",
    "            prompt += example.format_for_prompt(target_word)\n",
    "            prompt += \"\\n\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"<baseline>\n",
    "{self.text}\n",
    "</baseline>\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def parse_response(self, response: str):\n",
    "        # get the sentences inside <positive {TARGET_WORD}> and <negative {TARGET_WORD}>\n",
    "        try:\n",
    "            self.positive_variation = response.split(f\"<positive {TARGET_WORD}>\")[1].split(f\"</positive {TARGET_WORD}>\")[0].strip()\n",
    "            self.negative_variation = response.split(f\"<negative {TARGET_WORD}>\")[1].split(f\"</negative {TARGET_WORD}>\")[0].strip()\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"LLM response does not contain the expected format: {response}\")\n",
    "        return self.positive_variation, self.negative_variation\n",
    "\n",
    "    def get_variations(self) -> list[str]:\n",
    "        \"\"\" Returns a list of two strings: one where the TARGET_WORD has a more positive connotation, and one where it has a more negative one. \"\"\"\n",
    "        assert TARGET_WORD in self.text.lower(), f\"TARGET_WORD {TARGET_WORD} not found in {self.text}\"\n",
    "        prompt = self.get_prompt(TARGET_WORD)\n",
    "        res = sm.generate_text(prompt=prompt, llm_provider=PROVIDER, llm_model=MODEL)\n",
    "        return self.parse_response(res)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_baselines(word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES) -> List[str]:\n",
    "        # find the baselines.csv file in the `input` folder\n",
    "        filepath = os.path.join('input', 'baselines', f'{word}_{epoch}.baseline_1500_sentences.csv')\n",
    "        df = pd.read_csv(filepath)\n",
    "        # return the `sentence` column as a list\n",
    "        print(f\"Word {word}, epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Loaded {len(df)} baseline sentences, sampling {MAX_BASELINES}\")\n",
    "        baselines = df['sentence'].tolist()\n",
    "        if MAX_BASELINES and len(baselines) > MAX_BASELINES:\n",
    "            baselines = random.sample(baselines, MAX_BASELINES)\n",
    "        baselines = [s.replace(TARGET_WORD, TARGET_WORD) for s in baselines]\n",
    "        return baselines\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_sentences(sentences: List['SentenceToModify'], word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES):\n",
    "        output_file = os.path.join('output', f'{word}_{epoch}.synthetic_sentences.csv')\n",
    "        df = pd.DataFrame([{'baseline': s.text, 'positive_variation': s.positive_variation, 'negative_variation': s.negative_variation} for s in sentences])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(sentences)} sentences to {output_file}\")\n",
    "    \n",
    "\n",
    "baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "sentence = SentenceToModify(text=baselines[0])\n",
    "print(sentence.get_prompt(target_word=TARGET_WORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word trauma, epoch 1970-1974: Loaded 13 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1970-1974: 100%|██████████| 13/13 [00:22<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 13 sentences to output\\trauma_1970-1974.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1970-1974.synthetic_sentences.csv\n",
      "Word trauma, epoch 1975-1979: Loaded 11 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1975-1979: 100%|██████████| 11/11 [00:20<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11 sentences to output\\trauma_1975-1979.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1975-1979.synthetic_sentences.csv\n",
      "Word trauma, epoch 1980-1984: Loaded 51 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1980-1984: 100%|██████████| 51/51 [01:31<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 51 sentences to output\\trauma_1980-1984.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1980-1984.synthetic_sentences.csv\n",
      "Word trauma, epoch 1985-1989: Loaded 99 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1985-1989: 100%|██████████| 99/99 [02:37<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 99 sentences to output\\trauma_1985-1989.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1985-1989.synthetic_sentences.csv\n",
      "Word trauma, epoch 1990-1994: Loaded 297 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1990-1994: 100%|██████████| 297/297 [07:47<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 297 sentences to output\\trauma_1990-1994.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1990-1994.synthetic_sentences.csv\n",
      "Word trauma, epoch 1995-1999: Loaded 521 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 1995-1999: 100%|██████████| 521/521 [14:29<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 521 sentences to output\\trauma_1995-1999.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_1995-1999.synthetic_sentences.csv\n",
      "Word trauma, epoch 2000-2004: Loaded 650 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 2000-2004: 100%|██████████| 650/650 [18:01<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 650 sentences to output\\trauma_2000-2004.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_2000-2004.synthetic_sentences.csv\n",
      "Word trauma, epoch 2005-2009: Loaded 742 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 2005-2009: 100%|██████████| 742/742 [20:44<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 742 sentences to output\\trauma_2005-2009.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_2005-2009.synthetic_sentences.csv\n",
      "Word trauma, epoch 2010-2014: Loaded 552 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 2010-2014: 100%|██████████| 552/552 [15:48<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 552 sentences to output\\trauma_2010-2014.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_2010-2014.synthetic_sentences.csv\n",
      "Word trauma, epoch 2015-2019: Loaded 627 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trauma, 2015-2019: 100%|██████████| 627/627 [18:34<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 627 sentences to output\\trauma_2015-2019.synthetic_sentences.csv\n",
      "Processed and saved: output/5-year\\trauma_2015-2019.synthetic_sentences.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants for processing\n",
    "MAX_BASELINES = 1500\n",
    "OUTPUT_DIR = \"output/5-year\"  # Directory where processed files are saved\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through each target word and epoch\n",
    "for TARGET_WORD in get_args(TARGET_WORD_CHOICES):\n",
    "    for EPOCH in EPOCH_CHOICES:\n",
    "        # Construct the file path to check if it already exists\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{TARGET_WORD}_{EPOCH}.synthetic_sentences.csv\")\n",
    "        \n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {TARGET_WORD}, {EPOCH}: File already processed ({output_file} exists).\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure baselines exist before processing\n",
    "        baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "        if not baselines:\n",
    "            print(f\"No baselines found for {TARGET_WORD}, {EPOCH}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Process each baseline sentence\n",
    "        sentences = []\n",
    "        for baseline in tqdm(baselines, desc=f\"Processing {TARGET_WORD}, {EPOCH}\"):\n",
    "            sentence = SentenceToModify(text=baseline)\n",
    "            try:\n",
    "                positive_variation, negative_variation = sentence.get_variations()\n",
    "                sentences.append(sentence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sentence: {baseline}. Error: {str(e)}\")\n",
    "        \n",
    "        # Save the processed sentences\n",
    "        if sentences:  # Only save if there are completed sentences\n",
    "            SentenceToModify.save_sentences(sentences, word=TARGET_WORD, epoch=EPOCH)\n",
    "            print(f\"Processed and saved: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No valid sentences processed for {TARGET_WORD}, {EPOCH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation issues found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory containing the synthetic sentence files\n",
    "input_directory = \"output/5-year\"\n",
    "output_directory = \"output/validation_issues\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to validate that all rows in each column contain the target term\n",
    "def validate_target_in_files(directory):\n",
    "    issues_summary = []  # To store summary of issues\n",
    "    all_problematic_rows_combined = pd.DataFrame()  # To store all unique problematic rows across files\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            # Extract the target term from the filename\n",
    "            target_term = file_name.split(\"_\")[0]\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Check if the required columns exist\n",
    "                if {'baseline', 'positive_variation', 'negative_variation'}.issubset(df.columns):\n",
    "                    all_problematic_rows = pd.DataFrame()\n",
    "\n",
    "                    # Use a set to track rows we've already added to avoid duplicates\n",
    "                    seen_rows = set()\n",
    "\n",
    "                    for column in ['baseline', 'positive_variation', 'negative_variation']:\n",
    "                        # Identify rows that do not contain the target term\n",
    "                        problematic_rows = df[~df[column].str.contains(target_term, case=False, na=False)].copy()\n",
    "\n",
    "                        if not problematic_rows.empty:\n",
    "                            # Add the target term, epoch, and row number for identification\n",
    "                            problematic_rows['target'] = target_term  # Add the target term\n",
    "                            problematic_rows['epoch'] = file_name.split(\"_\")[1].split(\".\")[0]  # Extract epoch from filename\n",
    "                            problematic_rows['row_number'] = problematic_rows.index\n",
    "                            problematic_rows['problem_column'] = column\n",
    "\n",
    "                            # Only add rows that have not been added before (check via 'row_number')\n",
    "                            problematic_rows = problematic_rows[~problematic_rows['row_number'].isin(seen_rows)]\n",
    "                            seen_rows.update(problematic_rows['row_number'])\n",
    "\n",
    "                            # Append to the combined DataFrame for this file\n",
    "                            all_problematic_rows = pd.concat([all_problematic_rows, problematic_rows])\n",
    "\n",
    "                    # If there are problematic rows, save them to a new file\n",
    "                    if not all_problematic_rows.empty:\n",
    "                        # Combine with the master list of all rows across all files\n",
    "                        all_problematic_rows_combined = pd.concat([all_problematic_rows_combined, all_problematic_rows])\n",
    "\n",
    "                        issues_summary.append((file_name, len(all_problematic_rows)))\n",
    "\n",
    "                else:\n",
    "                    print(f\"File {file_name} is missing required columns.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    # Remove duplicates across the entire combined DataFrame\n",
    "    if not all_problematic_rows_combined.empty:\n",
    "        all_problematic_rows_combined = all_problematic_rows_combined.drop_duplicates(\n",
    "            subset=['baseline', 'positive_variation', 'negative_variation', 'row_number']\n",
    "        )\n",
    "\n",
    "        # Save the unique problematic rows to a new file\n",
    "        output_file = os.path.join(output_directory, f\"validation_issues_TEST.csv\")\n",
    "        all_problematic_rows_combined.to_csv(output_file, index=False)\n",
    "\n",
    "        # Print summary of issues\n",
    "        print(\"The following files have issues:\")\n",
    "        for file, count in issues_summary:\n",
    "            print(f\"File: {file}, Number of problematic rows: {count}\")\n",
    "        \n",
    "        # Display problematic rows in the notebook\n",
    "        print(\"\\nProblematic rows across all files:\")\n",
    "        display(all_problematic_rows_combined)  # This will display the dataframe in the notebook\n",
    "\n",
    "    else:\n",
    "        print(\"No validation issues found.\")\n",
    "\n",
    "# Run the validation\n",
    "validate_target_in_files(input_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording OpenAI API Credits Usage. Generating pos/neg variations for: \n",
    "- \"abuse\" = 696 - 679.36 = 16.64 USD\n",
    "- \"anxiety\" = 679.36 - 651.18 = 28 USD\n",
    "- \"depression\" = 651.18 - 621.84 = 29.34 USD\n",
    "- \"mental_health\" = 487.19 - 465.84 = 21 USD\n",
    "- \"mental_illness\" = 465.84 - 456.61 = 9 USD\n",
    "- \"trauma\" = 456.61 - 445.40 = 11 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and added file: abuse_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: abuse_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: abuse_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: abuse_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: abuse_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: abuse_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: abuse_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: abuse_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: abuse_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: abuse_2015-2019.synthetic_sentences.csv\n",
      "Files for 'abuse' merged successfully into 'output/all-year/abuse_synthetic_sentences.csv'\n",
      "Found and added file: anxiety_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2015-2019.synthetic_sentences.csv\n",
      "Files for 'anxiety' merged successfully into 'output/all-year/anxiety_synthetic_sentences.csv'\n",
      "Found and added file: depression_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: depression_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: depression_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: depression_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: depression_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: depression_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: depression_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: depression_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: depression_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: depression_2015-2019.synthetic_sentences.csv\n",
      "Files for 'depression' merged successfully into 'output/all-year/depression_synthetic_sentences.csv'\n",
      "Found and added file: mental_health_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2015-2019.synthetic_sentences.csv\n",
      "Files for 'mental_health' merged successfully into 'output/all-year/mental_health_synthetic_sentences.csv'\n",
      "Found and added file: mental_illness_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2015-2019.synthetic_sentences.csv\n",
      "Files for 'mental_illness' merged successfully into 'output/all-year/mental_illness_synthetic_sentences.csv'\n",
      "Found and added file: trauma_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: trauma_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: trauma_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: trauma_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: trauma_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: trauma_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: trauma_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: trauma_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: trauma_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: trauma_2015-2019.synthetic_sentences.csv\n",
      "Files for 'trauma' merged successfully into 'output/all-year/trauma_synthetic_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory where the CSV files are located\n",
    "input_directory = 'output/5-year'  # Folder with the 5-year CSV files\n",
    "output_directory = 'output/all-year'  # Folder to save the merged files\n",
    "\n",
    "# List of targets (you can add more targets to this list)\n",
    "targets = ['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Loop through each target and process the corresponding files\n",
    "for target in targets:\n",
    "    # Initialize an empty list to collect DataFrames for the current target\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.startswith(target) and file_name.endswith(\"synthetic_sentences.csv\"):  # Match files with the target\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_list.append(df)\n",
    "            print(f\"Found and added file: {file_name}\")  # Debugging print\n",
    "\n",
    "    # Check if any files were added to df_list\n",
    "    if not df_list:\n",
    "        print(f\"No files found for target: {target}\")  # Debugging print\n",
    "\n",
    "    # Concatenate all DataFrames for the current target into one\n",
    "    if df_list:\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Save the merged DataFrame to the new output folder\n",
    "        merged_df.to_csv(os.path.join(output_directory, f'{target}_synthetic_sentences.csv'), index=False)\n",
    "        print(f\"Files for '{target}' merged successfully into 'output/all-year/{target}_synthetic_sentences.csv'\")\n",
    "    else:\n",
    "        print(f\"Skipping '{target}' as no files were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
