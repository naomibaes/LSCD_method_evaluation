{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files have issues:\n",
      "File: abuse_1975-1979.synthetic_sentences.csv, Number of problematic rows: 3\n",
      "File: abuse_1980-1984.synthetic_sentences.csv, Number of problematic rows: 2\n",
      "File: abuse_1985-1989.synthetic_sentences.csv, Number of problematic rows: 5\n",
      "File: abuse_1990-1994.synthetic_sentences.csv, Number of problematic rows: 1\n",
      "File: abuse_1995-1999.synthetic_sentences.csv, Number of problematic rows: 1\n",
      "File: abuse_2000-2004.synthetic_sentences.csv, Number of problematic rows: 3\n",
      "File: abuse_2005-2009.synthetic_sentences.csv, Number of problematic rows: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory containing the synthetic sentence files\n",
    "input_directory = \"output\"\n",
    "output_directory = \"output/validation_issues\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to validate that all rows in each column contain the target term\n",
    "def validate_target_in_files(directory):\n",
    "    issues_summary = []  # To store summary of issues\n",
    "    all_problematic_rows_combined = pd.DataFrame()  # To store all unique problematic rows across files\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            # Extract the target term from the filename\n",
    "            target_term = file_name.split(\"_\")[0]\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Check if the required columns exist\n",
    "                if {'baseline', 'positive_variation', 'negative_variation'}.issubset(df.columns):\n",
    "                    all_problematic_rows = pd.DataFrame()\n",
    "\n",
    "                    # Use a set to track rows we've already added to avoid duplicates\n",
    "                    seen_rows = set()\n",
    "\n",
    "                    for column in ['baseline', 'positive_variation', 'negative_variation']:\n",
    "                        # Identify rows that do not contain the target term\n",
    "                        problematic_rows = df[~df[column].str.contains(target_term, case=False, na=False)].copy()\n",
    "\n",
    "                        if not problematic_rows.empty:\n",
    "                            # Add epoch and row number for identification\n",
    "                            problematic_rows['epoch'] = file_name.split(\"_\")[1].split(\".\")[0]  # Extract epoch from filename\n",
    "                            problematic_rows['row_number'] = problematic_rows.index\n",
    "                            problematic_rows['problem_column'] = column\n",
    "\n",
    "                            # Only add rows that have not been added before (check via 'row_number')\n",
    "                            problematic_rows = problematic_rows[~problematic_rows['row_number'].isin(seen_rows)]\n",
    "                            seen_rows.update(problematic_rows['row_number'])\n",
    "\n",
    "                            # Append to the combined DataFrame for this file\n",
    "                            all_problematic_rows = pd.concat([all_problematic_rows, problematic_rows])\n",
    "\n",
    "                    # If there are problematic rows, save them to a new file\n",
    "                    if not all_problematic_rows.empty:\n",
    "                        # Combine with the master list of all rows across all files\n",
    "                        all_problematic_rows_combined = pd.concat([all_problematic_rows_combined, all_problematic_rows])\n",
    "\n",
    "                        issues_summary.append((file_name, len(all_problematic_rows)))\n",
    "\n",
    "                else:\n",
    "                    print(f\"File {file_name} is missing required columns.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    # Remove duplicates across the entire combined DataFrame\n",
    "    if not all_problematic_rows_combined.empty:\n",
    "        all_problematic_rows_combined = all_problematic_rows_combined.drop_duplicates(subset=['baseline', 'positive_variation', 'negative_variation', 'row_number'])\n",
    "\n",
    "        # Save the unique problematic rows to a new file\n",
    "        output_file = os.path.join(output_directory, f\"validation_issues.csv\")\n",
    "        all_problematic_rows_combined.to_csv(output_file, index=False)\n",
    "\n",
    "        # Print summary of issues\n",
    "        print(\"The following files have issues:\")\n",
    "        for file, count in issues_summary:\n",
    "            print(f\"File: {file}, Number of problematic rows: {count}\")\n",
    "    else:\n",
    "        print(\"No validation issues found.\")\n",
    "\n",
    "# Run the validation\n",
    "validate_target_in_files(input_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
