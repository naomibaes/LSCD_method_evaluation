{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Intensity variations using LLMs\n",
    "\n",
    "Author: Raphael Merx (main) and Naomi Baes\n",
    "Input: a baseline sentence; Output: variations of this sentence where a target word is more or less intense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simplemind python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naomi\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import simplemind as sm\n",
    "from typing import Literal, List, get_args\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Define TARGET_WORD_CHOICES\n",
    "TARGET_WORD_CHOICES = Literal['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "TARGET_WORD: TARGET_WORD_CHOICES = 'abuse'\n",
    "\n",
    "# Generate human-readable versions for all target words\n",
    "TARGET_WORD_HUMAN_CHOICES = {word: word.replace('_', ' ') for word in get_args(TARGET_WORD_CHOICES)}\n",
    "TARGET_WORD_HUMAN = TARGET_WORD_HUMAN_CHOICES[TARGET_WORD]\n",
    "\n",
    "# Validate that both 'mental_health' and 'mental_illness' are handled correctly\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_health'] == 'mental health', \"Error: 'mental_health' not handled correctly\"\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_illness'] == 'mental illness', \"Error: 'mental_illness' not handled correctly\"\n",
    "\n",
    "# 1970-1974, ..., 2015-2019\n",
    "EPOCH_CHOICES = [f\"{y}-{y+4}\" for y in range(1970, 2020, 5)]\n",
    "EPOCH = EPOCH_CHOICES[0]\n",
    "\n",
    "MAX_BASELINES = 10\n",
    "\n",
    "# can be changed to gemini, see https://pypi.org/project/simplemind/\n",
    "PROVIDER = \"openai\"\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get neutral baseline sentences from corpus for LLM input for each target\n",
    "\n",
    "- **Script 1 Aim**: This script computes sentence-level sentiment scores using the NRC-VAD lexicon for a corpus of target terms spanning 1970–2019. It dynamically determines neutral sentiment ranges for each 5-year epoch by expanding outward from the median sentiment score within the interquartile range (Q1–Q3) until at least 500 sentences are included, capped at 1500. The selected sentences are saved to CSV files for each target and epoch, and a summary file logs the dynamic ranges and counts.\n",
    "\n",
    "- **Script 2 Aim**: This script processes pre-saved baseline CSV files to calculate sentence counts by year and 5-year epochs for multiple target terms. It generates \"year_count_lines.csv\" and \"epoch_count_lines.csv\" summarizing these counts and creates an epoch-based bar plot visualizing sentence distributions across the specified epochs for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\naomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\abuse.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\anxiety.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\depression.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\mental_health.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\mental_illness.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\trauma.lines.psych\n",
      "Neutral range summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\0_neutral_range_summary.csv.\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2015-2019.baseline_1500_sentences.csv\n",
      "Year count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\year_count_lines.csv.\n",
      "Epoch count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\epoch_count_lines.csv.\n",
      "Epoch plot saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\epoch_counts_intensity.png.\n"
     ]
    }
   ],
   "source": [
    "%run step0_get_neutral_baselines_intensity.py\n",
    "%run step1_plot_neutral_baselines_intensity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup examples to inject in the prompt\n",
    "\n",
    "-This code sets up examples of baseline sentences and their intensity-modified variations (more and less intense) to provide context and guidance for the LLM. \n",
    "-These examples are formatted into a structured prompt to help the model understand how to generate intensity-modified variations for new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    baseline: str\n",
    "    more_intense: str\n",
    "    less_intense: str\n",
    "\n",
    "    def format_for_prompt(self):\n",
    "        return f\"\"\"<baseline>\n",
    "{self.baseline}\n",
    "</baseline>\n",
    "<increased {TARGET_WORD} intensity>\n",
    "{self.more_intense}\n",
    "</increased {TARGET_WORD} intensity>\n",
    "<decreased {TARGET_WORD} intensity>\n",
    "{self.less_intense}\n",
    "</decreased {TARGET_WORD} intensity>\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def read_example_data(word: TARGET_WORD_CHOICES) -> pd.DataFrame:\n",
    "        filepath = os.path.join('input', f'intensity_example_sentences.xlsx')\n",
    "        # read to a dataframe\n",
    "        df = pd.read_excel(filepath)\n",
    "        # filter rows where `target` column is equal to the `word` argument\n",
    "        df = df[df['target'] == word]\n",
    "        print(f\"Loaded {len(df)} examples for the term '{word}'\")\n",
    "        return df\n",
    "\n",
    "def get_examples() -> List[Example]:\n",
    "    example_data = Example.read_example_data(TARGET_WORD)\n",
    "\n",
    "    return [\n",
    "        Example(\n",
    "            baseline=row['baseline'],\n",
    "            more_intense=row['high_intensity'],\n",
    "            less_intense=row['low_intensity']\n",
    "        )\n",
    "        for _, row in example_data.iterrows()\n",
    "    ]\n",
    "\n",
    "PROMPT_INTRO = \"\"\"In psychology research, Intensity is defined as “the degree to which a word has emotionally charged (i.e., strong, potent, high-arousal) connotations.” This task focuses on the intensity of the term **<<{target_word}>>**. \n",
    "\n",
    "### **Task**  \n",
    "You will be given a sentence containing the term **<<{target_word}>>**. Your goal is to write two new sentences:\n",
    "1. One where **<<{target_word}>>** is **less intense**.  \n",
    "2. One where **<<{target_word}>>** is **more intense**.  \n",
    "\n",
    "### **Rules**  \n",
    "1. The term **<<{target_word}>>** must remain **exactly as it appears** in the original sentence:\n",
    "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
    "   - Synonyms, variations, or altered spellings are not allowed.  \n",
    "\n",
    "2. **Meaning and Structure**:  \n",
    "   - Stay true to the original context and subject matter.  \n",
    "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
    "\n",
    "### **Important**  \n",
    "- Any response omitting, replacing, or altering **<<{target_word}>>** will be rejected.  \n",
    "- Ensure the output is:  \n",
    "   - **Grammatically correct**  \n",
    "   - **Sensitive and serious** in tone  \n",
    "   - **Free from exaggeration or sensationalism**  \n",
    "\n",
    "Follow these guidelines strictly to produce valid responses.  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word abuse, epoch 1970-1974: Loaded 13 baseline sentences, sampling 10\n",
      "Loaded 5 examples for the term 'abuse'\n",
      "In psychology research, Intensity is defined as “the degree to which a word has emotionally charged (i.e., strong, potent, high-arousal) connotations.” This task focuses on the intensity of the term **<<abuse>>**. \n",
      "\n",
      "### **Task**  \n",
      "You will be given a sentence containing the term **<<abuse>>**. Your goal is to write two new sentences:\n",
      "1. One where **<<abuse>>** is **less intense**.  \n",
      "2. One where **<<abuse>>** is **more intense**.  \n",
      "\n",
      "### **Rules**  \n",
      "1. The term **<<abuse>>** must remain **exactly as it appears** in the original sentence:\n",
      "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
      "   - Synonyms, variations, or altered spellings are not allowed.  \n",
      "\n",
      "2. **Meaning and Structure**:  \n",
      "   - Stay true to the original context and subject matter.  \n",
      "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
      "\n",
      "### **Important**  \n",
      "- Any response omitting, replacing, or altering **<<abuse>>** will be rejected.  \n",
      "- Ensure the output is:  \n",
      "   - **Grammatically correct**  \n",
      "   - **Sensitive and serious** in tone  \n",
      "   - **Free from exaggeration or sensationalism**  \n",
      "\n",
      "Follow these guidelines strictly to produce valid responses.  \n",
      "\n",
      "\n",
      "<baseline>\n",
      "Clinically, however, individual questions that use broad labeling terms are more likely to identify women as having a history of abuse.\n",
      "</baseline>\n",
      "<increased abuse intensity>\n",
      "Clinically, however, individual questions that use extreme labeling terms are more likely to reveal women as having a severe history of abuse.\n",
      "</increased abuse intensity>\n",
      "<decreased abuse intensity>\n",
      "Clinically, however, individual questions that use broad labeling terms are more likely to identify women as having a mild history of abuse.\n",
      "</decreased abuse intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Most care workers said that they would be willing to report abuse anonymously.\n",
      "</baseline>\n",
      "<increased abuse intensity>\n",
      "Most care workers cried that they would be delighted to report extreme instances of abuse anonymously.\n",
      "</increased abuse intensity>\n",
      "<decreased abuse intensity>\n",
      "Most care workers said that they would be willing to report trivial abuse anonymously.\n",
      "</decreased abuse intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "There is greater emphasis on recognizing that older people may be subjected to abuse and neglect by family members and the community as well.\n",
      "</baseline>\n",
      "<increased abuse intensity>\n",
      "There is a significant emphasis on recognizing that older people may be subjected to severe abuse and appalling neglect by family members and the community as well.\n",
      "</increased abuse intensity>\n",
      "<decreased abuse intensity>\n",
      "There is some emphasis on recognizing that older people may experience weak abuse by family members and the community as well.\n",
      "</decreased abuse intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Education on financial abuse for both elders and their adult children and establishment of income support programs are urgently needed.\n",
      "</baseline>\n",
      "<increased abuse intensity>\n",
      "Education on ordinary financial abuse for both elders and their adult children and urgent establishment of income support programs are desperately needed.\n",
      "</increased abuse intensity>\n",
      "<decreased abuse intensity>\n",
      "Education on financial abuse for both elders and their adult children and establishment of income support programs will occur.\n",
      "</decreased abuse intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "There was no association between physical abuse and depressive symptoms through either self-compassion or gratitude.\n",
      "</baseline>\n",
      "<increased abuse intensity>\n",
      "There was no association between frightening physical abuse and cold symptoms through either emotional contagion or extreme gratitude.\n",
      "</increased abuse intensity>\n",
      "<decreased abuse intensity>\n",
      "There was no association between mild physical abuse and state of mind through either complacency or gratitude.\n",
      "</decreased abuse intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Compared to the flow parameters in a control group (17 patients without alcohol abuse and diseases of the central nervous system) CBF was decreased significantly in the alcoholics, and this decrease was dependent from the severity of the clinical syndrome and of the mental disfunction.\n",
      "</baseline>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SentenceToModify:\n",
    "    text: str\n",
    "    increased_variation: str = None\n",
    "    decreased_variation: str = None\n",
    "\n",
    "    def get_prompt(self):\n",
    "        prompt = PROMPT_INTRO.format(target_word=TARGET_WORD) + \"\\n\\n\"\n",
    "        for example in get_examples():\n",
    "            prompt += example.format_for_prompt()\n",
    "            prompt += \"\\n\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"<baseline>\n",
    "{self.text}\n",
    "</baseline>\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def parse_response(self, response: str):\n",
    "        # get the sentences inside <more {TARGET_WORD}> and <less {TARGET_WORD}>\n",
    "        self.increased_variation = response.split(f\"<increased {TARGET_WORD} intensity>\")[1].split(f\"</increased {TARGET_WORD} intensity>\")[0].strip()\n",
    "        self.decreased_variation = response.split(f\"<decreased {TARGET_WORD} intensity>\")[1].split(f\"</decreased {TARGET_WORD} intensity>\")[0].strip()\n",
    "        return self.increased_variation, self.decreased_variation\n",
    "\n",
    "    def get_variations(self) -> list[str]:\n",
    "        \"\"\" Returns a list of two strings: one where the TARGET_WORD is more intense, and one where it is less intense \"\"\"\n",
    "        assert TARGET_WORD in self.text, f\"word {TARGET_WORD} not found in {self.text}\"\n",
    "        prompt = self.get_prompt()\n",
    "        res = sm.generate_text(prompt=prompt, llm_provider=PROVIDER, llm_model=MODEL)\n",
    "        return self.parse_response(res)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_baselines(word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES) -> List[str]:\n",
    "        # find the baselines.csv file in the `input` folder\n",
    "        filepath = os.path.join('input', 'baselines', f'{word}_{epoch}.baseline_1500_sentences.csv')\n",
    "        df = pd.read_csv(filepath)\n",
    "        # return the `sentence` column as a list\n",
    "        print(f\"Word {word}, epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Loaded {len(df)} baseline sentences, sampling {MAX_BASELINES}\")\n",
    "        baselines = df['sentence'].tolist()\n",
    "        if MAX_BASELINES and len(baselines) > MAX_BASELINES:\n",
    "            baselines = random.sample(baselines, MAX_BASELINES)\n",
    "        baselines = [s.replace(TARGET_WORD, TARGET_WORD) for s in baselines]\n",
    "        return baselines\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_sentences(sentences: List['SentenceToModify'], word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES):\n",
    "        # Adjust the directory here to include 'test'\n",
    "        output_dir = 'output/test'\n",
    "        output_file = os.path.join(output_dir, f'{word}_{epoch}.synthetic_sentences.csv')\n",
    "        \n",
    "        # Ensure the directory exists before trying to write to it\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Creating DataFrame and saving to CSV\n",
    "        df = pd.DataFrame([{'baseline': s.text, 'high_intensity': s.increased_variation, 'low_intensity': s.decreased_variation} for s in sentences])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(sentences)} sentences to {output_file}\")\n",
    "\n",
    "baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "sentence = SentenceToModify(text=baselines[0])\n",
    "print(sentence.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if output/test\\abuse_1970-1974.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1970-1974: File already processed.\n",
      "Checking if output/test\\abuse_1975-1979.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1975-1979: File already processed.\n",
      "Checking if output/test\\abuse_1980-1984.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1980-1984: File already processed.\n",
      "Checking if output/test\\abuse_1985-1989.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1985-1989: File already processed.\n",
      "Checking if output/test\\abuse_1990-1994.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1990-1994: File already processed.\n",
      "Checking if output/test\\abuse_1995-1999.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 1995-1999: File already processed.\n",
      "Checking if output/test\\abuse_2000-2004.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 2000-2004: File already processed.\n",
      "Checking if output/test\\abuse_2005-2009.synthetic_sentences.csv exists...\n",
      "Skipping abuse, 2005-2009: File already processed.\n",
      "Checking if output/test\\abuse_2010-2014.synthetic_sentences.csv exists...\n",
      "Word abuse, epoch 2010-2014: Loaded 1111 baseline sentences, sampling 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:19,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:05<00:11,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:09,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:08<00:08,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:10<00:06,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:11<00:05,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:13<00:03,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:14<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 sentences to output/test\\abuse_2010-2014.synthetic_sentences.csv\n",
      "Processed and saved: output/test\\abuse_2010-2014.synthetic_sentences.csv\n",
      "Checking if output/test\\abuse_2015-2019.synthetic_sentences.csv exists...\n",
      "Word abuse, epoch 2015-2019: Loaded 1079 baseline sentences, sampling 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:15,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:04<00:19,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:06<00:14,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:09<00:08,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:11<00:06,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:13<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:14<00:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:16<00:01,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'abuse'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 sentences to output/test\\abuse_2015-2019.synthetic_sentences.csv\n",
      "Processed and saved: output/test\\abuse_2015-2019.synthetic_sentences.csv\n",
      "Checking if output/test\\anxiety_1970-1974.synthetic_sentences.csv exists...\n",
      "Word anxiety, epoch 1970-1974: Loaded 375 baseline sentences, sampling 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples for the term 'anxiety'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sentence: In the case of a man with an acute onset of stuttering and massive free-floating anxiety following an automobile accident, a variety of behavioral techniques, including relaxation training, assertive training, graded rehearsal and modification of behavioral operants were employed, with the complete eradication of the symptoms in five 50-minute sessions.. Error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Save the processed sentences\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentences:  \u001b[38;5;66;03m# Only save if there are completed sentences\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m sentence \u001b[38;5;241m=\u001b[39m SentenceToModify(text\u001b[38;5;241m=\u001b[39mbaseline)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     more_intense_variation, less_intense_variation \u001b[38;5;241m=\u001b[39m \u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(sentence)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mSentenceToModify.get_variations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prompt()\n\u001b[0;32m     29\u001b[0m res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mgenerate_text(prompt\u001b[38;5;241m=\u001b[39mprompt, llm_provider\u001b[38;5;241m=\u001b[39mPROVIDER, llm_model\u001b[38;5;241m=\u001b[39mMODEL)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mSentenceToModify.parse_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# get the sentences inside <more {TARGET_WORD}> and <less {TARGET_WORD}>\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincreased_variation \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<increased \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTARGET_WORD\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m intensity>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</increased \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_WORD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m intensity>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecreased_variation \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<decreased \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_WORD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m intensity>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</decreased \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_WORD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m intensity>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincreased_variation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecreased_variation\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants for processing\n",
    "MAX_BASELINES = 10\n",
    "OUTPUT_DIR = \"output/test\"  # Directory where processed files are saved\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through each target word and epoch\n",
    "for TARGET_WORD in get_args(TARGET_WORD_CHOICES):\n",
    "    for EPOCH in EPOCH_CHOICES:\n",
    "        # Construct the file path to check if it already exists\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{TARGET_WORD}_{EPOCH}.synthetic_sentences.csv\")\n",
    "\n",
    "        # Print the file path to debug\n",
    "        print(f\"Checking if {output_file} exists...\")\n",
    "\n",
    "        # Skip if the file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {TARGET_WORD}, {EPOCH}: File already processed.\")\n",
    "            continue\n",
    "\n",
    "        # Load baselines if the file does not exist\n",
    "        baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "        sentences = []\n",
    "\n",
    "        # Process each baseline sentence\n",
    "        for baseline in tqdm(baselines, total=len(baselines), unit='it', leave=True):\n",
    "            sentence = SentenceToModify(text=baseline)\n",
    "            try:\n",
    "                more_intense_variation, less_intense_variation = sentence.get_variations()\n",
    "                sentences.append(sentence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sentence: {baseline}. Error: {str(e)}\")\n",
    "                raise e\n",
    "\n",
    "        # Save the processed sentences\n",
    "        if sentences:  # Only save if there are completed sentences\n",
    "            SentenceToModify.save_sentences(sentences, word=TARGET_WORD, epoch=EPOCH)\n",
    "            print(f\"Processed and saved: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No valid sentences processed for {TARGET_WORD}, {EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
