{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Intensity variations using LLMs\n",
    "\n",
    "Author: Raphael Merx (main) and Naomi Baes\n",
    "Input: a baseline sentence; Output: variations of this sentence where a target word is more or less intense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simplemind python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import simplemind as sm\n",
    "from typing import Literal, List\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "TARGET_WORD_CHOICES = Literal['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "\n",
    "TARGET_WORD: TARGET_WORD_CHOICES = 'mental_illness'\n",
    "TARGET_WORD_HUMAN = TARGET_WORD.replace('_', ' ') \n",
    "\n",
    "# 1970-1974, ..., 2015-2019\n",
    "EPOCH_CHOICES = [f\"{y}-{y+4}\" for y in range(1970, 2020, 5)]\n",
    "EPOCH = EPOCH_CHOICES[1]\n",
    "\n",
    "MAX_BASELINES = 10\n",
    "\n",
    "# can be changed to gemini, see https://pypi.org/project/simplemind/\n",
    "PROVIDER = \"openai\"\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get neutral baseline sentences from corpus for LLM input for each target\n",
    "\n",
    "- **Script 1 Aim**: This script computes sentence-level sentiment scores using the NRC-VAD lexicon for a corpus of target terms spanning 1970–2019. It dynamically determines neutral sentiment ranges for each 5-year epoch by expanding outward from the median sentiment score within the interquartile range (Q1–Q3) until at least 500 sentences are included, capped at 1500. The selected sentences are saved to CSV files for each target and epoch, and a summary file logs the dynamic ranges and counts.\n",
    "\n",
    "- **Script 2 Aim**: This script processes pre-saved baseline CSV files to calculate sentence counts by year and 5-year epochs for multiple target terms. It generates \"year_count_lines.csv\" and \"epoch_count_lines.csv\" summarizing these counts and creates an epoch-based bar plot visualizing sentence distributions across the specified epochs for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\naomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\abuse.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\anxiety.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\depression.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\mental_health.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\mental_illness.lines.psych\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\0.0_corpus_preprocessing\\output\\natural_lines_targets\\trauma.lines.psych\n",
      "Neutral range summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\0_neutral_range_summary.csv.\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\abuse_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\anxiety_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\depression_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_health_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\mental_illness_2015-2019.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1970-1974.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1975-1979.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1980-1984.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1985-1989.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1990-1994.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_1995-1999.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2000-2004.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2005-2009.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2010-2014.baseline_1500_sentences.csv\n",
      "Processing file: c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\trauma_2015-2019.baseline_1500_sentences.csv\n",
      "Year count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\year_count_lines.csv.\n",
      "Epoch count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\epoch_count_lines.csv.\n",
      "Epoch plot saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\epoch_counts_intensity.png.\n"
     ]
    }
   ],
   "source": [
    "%run step0_get_neutral_baselines_intensity.py\n",
    "%run step1_plot_neutral_baselines_intensity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup examples to inject in the prompt\n",
    "\n",
    "-This code sets up examples of baseline sentences and their intensity-modified variations (more and less intense) to provide context and guidance for the LLM. \n",
    "-These examples are formatted into a structured prompt to help the model understand how to generate intensity-modified variations for new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This code sets up examples of baseline sentences and their intensity-modified variations (more and less intense) to provide context and guidance for the LLM. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# These examples are formatted into a structured prompt to help the model understand how to generate intensity-modified variations for new sentences.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mExample\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     baseline: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m      7\u001b[0m     more_intense: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    baseline: str\n",
    "    more_intense: str\n",
    "    less_intense: str\n",
    "\n",
    "    def format_for_prompt(self):\n",
    "        return f\"\"\"<baseline>\n",
    "{self.baseline}\n",
    "</baseline>\n",
    "<increased {TARGET_WORD_HUMAN} intensity>\n",
    "{self.more_intense}\n",
    "</increased {TARGET_WORD_HUMAN} intensity>\n",
    "<decreased {TARGET_WORD_HUMAN} intensity>\n",
    "{self.less_intense}\n",
    "</decreased {TARGET_WORD_HUMAN} intensity>\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def read_example_data(word: TARGET_WORD_CHOICES) -> pd.DataFrame:\n",
    "        filepath = os.path.join('input', f'intensity_example_sentences.xlsx')\n",
    "        # read to a dataframe\n",
    "        df = pd.read_excel(filepath)\n",
    "        # filter rows where `target` column is equal to the `word` argument\n",
    "        df = df[df['target'] == word]\n",
    "        print(f\"Loaded {len(df)} examples for the term '{word}'\")\n",
    "        return df\n",
    "\n",
    "example_data = Example.read_example_data(TARGET_WORD)\n",
    "\n",
    "EXAMPLES = [\n",
    "    Example(\n",
    "        baseline=row['baseline'],\n",
    "        more_intense=row['high_intensity'],\n",
    "        less_intense=row['low_intensity']\n",
    "    )\n",
    "    for _, row in example_data.iterrows()\n",
    "]\n",
    "\n",
    "PROMPT_INTRO = f\"\"\"In psychology research, \"Intensity\" is defined as “the degree to which a word has emotionally charged \n",
    "(i.e., strong, potent, high-arousal) connotations.” Here we study the intensity of the term \"{TARGET_WORD_HUMAN}\".\n",
    "\n",
    "You will be given a sentence with the word \"{TARGET_WORD_HUMAN}\" in it. You will then be asked to write two new sentences: \n",
    "One where the word \"{TARGET_WORD_HUMAN}\" is more intense, and one where it is less intense.\n",
    "\n",
    "Important Guidelines:\n",
    "- Retain the original structure and context of the baseline sentence as much as possible.\n",
    "- Make only small, targeted adjustments to alter the intensity of the term \"{TARGET_WORD_HUMAN}\". \n",
    "- Do not add or remove key concepts or introduce entirely new elements not implied in the baseline.\n",
    "- Do not replace the target term with another word.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 680 baseline sentences, sampling 10\n",
      "In psychology lexicographic research, we define \"intensity\" as the \"extent to which a word refers to more emotionally or referentially intense phenomena\". Here we study the intensity of the word \"mental illness\"\n",
      "You will be given a sentence with the word \"mental illness\" in it. You will then be asked to write two new sentences: one where the word \"mental illness\" is more intense, and one where it is less intense.\n",
      "\n",
      "<baseline>\n",
      "The study examined the prevalence of mental illness in urban populations.\n",
      "</baseline>\n",
      "<increased mental illness intensity>\n",
      "The study highlighted the staggering prevalence of severe mental illness in urban populations.\n",
      "</increased mental illness intensity>\n",
      "<decreased mental illness intensity>\n",
      "The study explored the relatively low prevalence of mild mental illness in urban populations.\n",
      "</decreased mental illness intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The research analyzed the treatment of mental illness in clinical settings.\n",
      "</baseline>\n",
      "<increased mental illness intensity>\n",
      "The research analyzed the treatment of severe and chronic mental illness in clinical settings.\n",
      "</increased mental illness intensity>\n",
      "<decreased mental illness intensity>\n",
      "The research analyzed the treatment of mild and short-term mental illness in clinical settings.\n",
      "</decreased mental illness intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The article discussed the stigma surrounding mental illness in society.\n",
      "</baseline>\n",
      "<increased mental illness intensity>\n",
      "The article discussed the pervasive stigma surrounding severe mental illness in society.\n",
      "</increased mental illness intensity>\n",
      "<decreased mental illness intensity>\n",
      "The article discussed the occasional stigma associated with mild mental illness in society.\n",
      "</decreased mental illness intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The findings revealed patterns of mental illness among marginalized groups.\n",
      "</baseline>\n",
      "<increased mental illness intensity>\n",
      "The findings revealed patterns of extreme and untreated mental illness among marginalized groups.\n",
      "</increased mental illness intensity>\n",
      "<decreased mental illness intensity>\n",
      "The findings revealed patterns of manageable and partially treated mental illness among marginalized groups.\n",
      "</decreased mental illness intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The investigation focused on mental illness and its impact on family dynamics.\n",
      "</baseline>\n",
      "<increased mental illness intensity>\n",
      "The investigation focused on the devastating impact of mental illness in disrupting family dynamics.\n",
      "</increased mental illness intensity>\n",
      "<decreased mental illness intensity>\n",
      "The investigation focused on the minor impact of mental illness in occasionally straining family dynamics.\n",
      "</decreased mental illness intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "Based on intergroup contact theory, a proposed comprehensive model of attitudes towards seeking professional psychological help was tested, including both potential barriers to mental_health help-seeking (i.e., public stigma and self-stigma of seeking help, prejudicial and essentialist beliefs about mental illness, intergroup anxiety) and potential facilitators (i.e., direct and extended contact with persons with mental illness).\n",
      "</baseline>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code chunk constructs a structured prompt by combining predefined examples with a new target sentence, queries an LLM to generate intensity-modified variations (more and less intense) \n",
    "# of the target sentence, and extracts the results for further analysis.\n",
    "\n",
    "@dataclass\n",
    "class SentenceToModify:\n",
    "    text: str\n",
    "    increased_variation: str = None\n",
    "    decreased_variation: str = None\n",
    "\n",
    "    def get_prompt(self):\n",
    "        prompt = PROMPT_INTRO\n",
    "        for example in EXAMPLES:\n",
    "            prompt += example.format_for_prompt()\n",
    "            prompt += \"\\n\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"<baseline>\n",
    "{self.text}\n",
    "</baseline>\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def parse_response(self, response: str):\n",
    "        # get the sentences inside <more {TARGET_WORD}> and <less {TARGET_WORD}>\n",
    "        self.increased_variation = response.split(f\"<increased {TARGET_WORD_HUMAN} intensity>\")[1].split(f\"</increased {TARGET_WORD_HUMAN} intensity>\")[0].strip()\n",
    "        self.decreased_variation = response.split(f\"<decreased {TARGET_WORD_HUMAN} intensity>\")[1].split(f\"</decreased {TARGET_WORD_HUMAN} intensity>\")[0].strip()\n",
    "        return self.increased_variation, self.decreased_variation\n",
    "\n",
    "    def get_variations(self) -> list[str]:\n",
    "        \"\"\" Returns a list of two strings: one where the TARGET_WORD is more intense, and one where it is less intense \"\"\"\n",
    "        assert TARGET_WORD_HUMAN in self.text, f\"word {TARGET_WORD_HUMAN} not found in {self.text}\"\n",
    "        prompt = self.get_prompt()\n",
    "        res = sm.generate_text(prompt=prompt, llm_provider=PROVIDER, llm_model=MODEL)\n",
    "        return self.parse_response(res)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_baselines(word: TARGET_WORD_CHOICES) -> List[str]:\n",
    "        # find the baselines.csv file in the `input` folder\n",
    "        filepath = os.path.join('input', 'baselines', f'{word}_neutral_baselines_intensity.csv')\n",
    "        df = pd.read_csv(filepath)\n",
    "        # return the `sentence` column as a list\n",
    "        print(f\"Loaded a total of {len(df)} baseline sentences, sampling {MAX_BASELINES}\")\n",
    "        baselines = df['sentence'].tolist()\n",
    "        baselines = random.sample(baselines, MAX_BASELINES) if len(baselines) > MAX_BASELINES else baselines\n",
    "        baselines = [s.replace(TARGET_WORD, TARGET_WORD_HUMAN) for s in baselines]\n",
    "        return baselines\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_sentences(sentences: List['SentenceToModify']):\n",
    "        output_file = os.path.join('output', f'{TARGET_WORD}_synthetic_sentences.csv')\n",
    "        df = pd.DataFrame([{'baseline': s.text, 'high_intensity': s.increased_variation, 'low_intensity': s.decreased_variation} for s in sentences])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(sentences)} sentences to {output_file}\")\n",
    "\n",
    "baselines = SentenceToModify.load_baselines(TARGET_WORD)\n",
    "sentence = SentenceToModify(text=baselines[0])\n",
    "print(sentence.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 sentences to output/mental_illness_synthetic_sentences.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for baseline in tqdm(baselines):\n",
    "    sentence = SentenceToModify(text=baseline)\n",
    "    positive_variation, negative_variation = sentence.get_variations()\n",
    "    sentences.append(sentence)\n",
    "\n",
    "SentenceToModify.save_sentences(sentences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
