{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Intensity variations using LLMs\n",
    "\n",
    "Author: Raphael Merx\n",
    "\n",
    "Input: A baseline sentence\n",
    "Output: variations of this sentence where a target word is more or less intense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simplemind python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import simplemind as sm\n",
    "from typing import Literal, List, get_args\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Define TARGET_WORD_CHOICES\n",
    "TARGET_WORD_CHOICES = Literal['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "TARGET_WORD: TARGET_WORD_CHOICES = 'abuse'\n",
    "\n",
    "# Generate human-readable versions for all target words\n",
    "TARGET_WORD_HUMAN_CHOICES = {word: word.replace('_', ' ') for word in get_args(TARGET_WORD_CHOICES)}\n",
    "TARGET_WORD_HUMAN = TARGET_WORD_HUMAN_CHOICES[TARGET_WORD]\n",
    "\n",
    "# Validate that both 'mental_health' and 'mental_illness' are handled correctly\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_health'] == 'mental health', \"Error: 'mental_health' not handled correctly\"\n",
    "assert TARGET_WORD_HUMAN_CHOICES['mental_illness'] == 'mental illness', \"Error: 'mental_illness' not handled correctly\"\n",
    "\n",
    "# 1970-1974, ..., 2015-2019\n",
    "EPOCH_CHOICES = [f\"{y}-{y+4}\" for y in range(1970, 2020, 5)]\n",
    "EPOCH = EPOCH_CHOICES[0]\n",
    "\n",
    "MAX_BASELINES = 1500\n",
    "\n",
    "# can be changed to gemini, see https://pypi.org/project/simplemind/\n",
    "PROVIDER = \"openai\"\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get neutral baseline sentences from corpus for LLM input for each target\n",
    "\n",
    "- **Script 1 Aim**: This script computes sentence-level sentiment scores using the NRC-VAD lexicon for a corpus of target terms spanning 1970–2019. It dynamically determines neutral sentiment ranges for each 5-year epoch by expanding outward from the median sentiment score within the interquartile range (Q1–Q3) until at least 500 sentences are included, capped at 1500. The selected sentences are saved to CSV files for each target and epoch, and a summary file logs the dynamic ranges and counts.\n",
    "\n",
    "- **Script 2 Aim**: This script processes pre-saved baseline CSV files to calculate sentence counts by year and 5-year epochs for multiple target terms. It generates \"year_count_lines.csv\" and \"epoch_count_lines.csv\" summarizing these counts and creates an epoch-based bar plot visualizing sentence distributions across the specified epochs for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\year_count_lines.csv.\n",
      "Epoch count summary saved to c:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\input\\baselines\\output\\epoch_count_lines.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\OneDrive\\COMP80004_PhDResearch\\RESEARCH\\PROJECTS\\3_evaluation+validation - ACL 2025\\ICL\\3_intensity\\synthetic\\step1_plot_neutral_baselines_intensity.py:75: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(target_data[\"epoch\"], rotation=45, ha=\"right\", fontsize=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch plot saved to ../../figures\\plot_appendixB_intensity.png.\n"
     ]
    }
   ],
   "source": [
    "#%run step0_get_neutral_baselines_intensity.py\n",
    "%run step1_plot_neutral_baselines_intensity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup examples to inject in the prompt\n",
    "\n",
    "-This code sets up examples of baseline sentences and their intensity-modified variations (more and less intense) to provide context and guidance for the LLM. \n",
    "-These examples are formatted into a structured prompt to help the model understand how to generate intensity-modified variations for new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    baseline: str\n",
    "    more_intense: str\n",
    "    less_intense: str\n",
    "\n",
    "    def format_for_prompt(self):\n",
    "        return f\"\"\"<baseline>\n",
    "{self.baseline}\n",
    "</baseline>\n",
    "<increased {TARGET_WORD} intensity>\n",
    "{self.more_intense}\n",
    "</increased {TARGET_WORD} intensity>\n",
    "<decreased {TARGET_WORD} intensity>\n",
    "{self.less_intense}\n",
    "</decreased {TARGET_WORD} intensity>\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def read_example_data(word: TARGET_WORD_CHOICES) -> pd.DataFrame:\n",
    "        filepath = os.path.join('input', f'intensity_example_sentences.xlsx')\n",
    "        # read to a dataframe\n",
    "        df = pd.read_excel(filepath)\n",
    "        # filter rows where `target` column is equal to the `word` argument\n",
    "        df = df[df['target'] == word]\n",
    "        return df\n",
    "\n",
    "def get_examples() -> List[Example]:\n",
    "    example_data = Example.read_example_data(TARGET_WORD)\n",
    "\n",
    "    return [\n",
    "        Example(\n",
    "            baseline=row['baseline'],\n",
    "            more_intense=row['high_intensity'],\n",
    "            less_intense=row['low_intensity']\n",
    "        )\n",
    "        for _, row in example_data.iterrows()\n",
    "    ]\n",
    "\n",
    "PROMPT_INTRO = \"\"\"In psychology research, Intensity is defined as “the degree to which a word has emotionally charged (i.e., strong, potent, high-arousal) connotations.” This task focuses on the intensity of the term **<<{target_word}>>**. \n",
    "\n",
    "### **Task**  \n",
    "You will be given a sentence containing the term **<<{target_word}>>**. Your goal is to write two new sentences:\n",
    "1. One where **<<{target_word}>>** is **less intense** (enclose this sentence between `<decreased {target_word} intensity>` and `</decreased {target_word} intensity>` tags).\n",
    "2. One where **<<{target_word}>>** is **more intense** (enclose this sentence between `<increased {target_word} intensity>` and `</increased {target_word} intensity>` tags).\n",
    "\n",
    "### **Rules**  \n",
    "1. The term **<<{target_word}>>** must remain **exactly as it appears** in the original sentence:\n",
    "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
    "   - Synonyms, variations, or altered spellings are not allowed.  \n",
    "\n",
    "2. **Meaning and Structure**:  \n",
    "   - Stay true to the original context and subject matter.  \n",
    "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
    "\n",
    "### **Important**  \n",
    "- Any response omitting, replacing, or altering **<<{target_word}>>** will be rejected.  \n",
    "- Ensure the output is:  \n",
    "   - **Grammatically correct**  \n",
    "   - **Sensitive and serious** in tone  \n",
    "   - **Free from exaggeration or sensationalism**  \n",
    "   - **Strictly following the XML-like tag format for intensity variations**\n",
    "\n",
    "Follow these guidelines strictly to produce valid responses.  \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word trauma, epoch 1970-1974: Loaded 12 baseline sentences, sampling 1500\n",
      "In psychology research, Intensity is defined as “the degree to which a word has emotionally charged (i.e., strong, potent, high-arousal) connotations.” This task focuses on the intensity of the term **<<trauma>>**. \n",
      "\n",
      "### **Task**  \n",
      "You will be given a sentence containing the term **<<trauma>>**. Your goal is to write two new sentences:\n",
      "1. One where **<<trauma>>** is **less intense** (enclose this sentence between `<decreased trauma intensity>` and `</decreased trauma intensity>` tags).\n",
      "2. One where **<<trauma>>** is **more intense** (enclose this sentence between `<increased trauma intensity>` and `</increased trauma intensity>` tags).\n",
      "\n",
      "### **Rules**  \n",
      "1. The term **<<trauma>>** must remain **exactly as it appears** in the original sentence:\n",
      "   - Do **not** replace, rephrase, omit, or modify it in any way.\n",
      "   - Synonyms, variations, or altered spellings are not allowed.  \n",
      "\n",
      "2. **Meaning and Structure**:  \n",
      "   - Stay true to the original context and subject matter.  \n",
      "   - Maintain the sentence’s structure and ensure grammatical accuracy.  \n",
      "\n",
      "### **Important**  \n",
      "- Any response omitting, replacing, or altering **<<trauma>>** will be rejected.  \n",
      "- Ensure the output is:  \n",
      "   - **Grammatically correct**  \n",
      "   - **Sensitive and serious** in tone  \n",
      "   - **Free from exaggeration or sensationalism**  \n",
      "   - **Strictly following the XML-like tag format for intensity variations**\n",
      "\n",
      "Follow these guidelines strictly to produce valid responses.  \n",
      "\n",
      "\n",
      "<baseline>\n",
      "They tend to be more liberal in their attitudes toward abortion than women in general; however, women who experienced a greater degree of psychic trauma tended to be more conservative in their attitudes.\n",
      "</baseline>\n",
      "<increased trauma intensity>\n",
      "They tend to be more extremely callous in their attitudes toward the horrors of abortion than women in general; however, women who suffered a greater degree of violent psychic trauma tended to be more fearful in their attitudes.\n",
      "</increased trauma intensity>\n",
      "<decreased trauma intensity>\n",
      "They tend to be more accepting in their attitudes toward children than women in general; however, women who experienced mild psychic trauma tended to be more conservative in their attitudes.\n",
      "</decreased trauma intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The trauma was overwhelming.\n",
      "</baseline>\n",
      "<increased trauma intensity>\n",
      "The intense trauma was completely overwhelming.\n",
      "</increased trauma intensity>\n",
      "<decreased trauma intensity>\n",
      "The mild trauma was unproblematic.\n",
      "</decreased trauma intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The choice of defensive style was found related to at least three factors: an early history of trauma, especially separation, parental encouragement of toughness, and essentially a counterphobic family style.\n",
      "</baseline>\n",
      "<increased trauma intensity>\n",
      "The choice of emotional overreaction was found related to at least three factors: an early history of extreme trauma, especially harsh abandonment, parental punishment, and essentially an emotionally destructive family style.\n",
      "</increased trauma intensity>\n",
      "<decreased trauma intensity>\n",
      "The choice of coping style was found related to at least three factors: an early history of mild trauma, especially independence, parental encouragement, and essentially a dull and normal family style.\n",
      "</decreased trauma intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "It is an attempt to bring the trauma arising from the external world into the internal world and thus to create an illusion of mastery and control.\n",
      "</baseline>\n",
      "<increased trauma intensity>\n",
      "It is a desperate attempt to bring the unbearable trauma threatening from the external world into the internal world and thus to create a poisonous illusion of mastery and control.\n",
      "</increased trauma intensity>\n",
      "<decreased trauma intensity>\n",
      "It is an attempt to bring the mild trauma arising from the external world into the internal world and thus to create a sense of peace and tranquillity.\n",
      "</decreased trauma intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The international standard for setting ski bindings is based on the measurement of the tibia proximal width because of the propensity of this bone to suffer trauma as the ski and skier attempt to go in different directions.\n",
      "</baseline>\n",
      "<increased trauma intensity>\n",
      "The disgraceful international standard for setting ski bindings is based on the measurement of the tibia proximal width because of the scary propensity of this bone to suffer severe trauma as the ski and skier attempt to go in different directions.\n",
      "</increased trauma intensity>\n",
      "<decreased trauma intensity>\n",
      "The international standard for setting ski bindings is based on the measurement of the tibia proximal width because of the propensity of this bone to experience mild trauma as the ski and skier attempt to go in different directions.\n",
      "</decreased trauma intensity>\n",
      "\n",
      "\n",
      "<baseline>\n",
      "The choice of defensive style was found related to at least three factors: an early history of trauma, especially separation, parental encouragement of toughness, and essentially a counterphobic family style.\n",
      "</baseline>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SentenceToModify:\n",
    "    text: str\n",
    "    increased_variation: str = None\n",
    "    decreased_variation: str = None\n",
    "\n",
    "    def get_prompt(self):\n",
    "        prompt = PROMPT_INTRO.format(target_word=TARGET_WORD) + \"\\n\\n\"\n",
    "        for example in get_examples():\n",
    "            prompt += example.format_for_prompt()\n",
    "            prompt += \"\\n\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"<baseline>\n",
    "{self.text}\n",
    "</baseline>\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def parse_response(self, response: str):\n",
    "        pattern = fr\"<increased {TARGET_WORD} intensity>(.*?)</increased {TARGET_WORD} intensity>\"\n",
    "        try:\n",
    "            self.increased_variation = re.search(pattern, response, re.DOTALL).group(1).strip()\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"LLM response does not contain the expected increased intensity format: {response}\")\n",
    "\n",
    "        pattern = fr\"<decreased {TARGET_WORD} intensity>(.*?)</decreased {TARGET_WORD} intensity>\"\n",
    "        try:\n",
    "            self.decreased_variation = re.search(pattern, response, re.DOTALL).group(1).strip()\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"LLM response does not contain the expected decreased intensity format: {response}\")\n",
    "\n",
    "        return self.increased_variation, self.decreased_variation\n",
    "\n",
    "    def get_variations(self) -> list[str]:\n",
    "        \"\"\" Returns a list of two strings: one where the TARGET_WORD is more intense, and one where it is less intense \"\"\"\n",
    "        assert TARGET_WORD in self.text.lower(), f\"word {TARGET_WORD} not found in {self.text}\"\n",
    "        prompt = self.get_prompt()\n",
    "        res = sm.generate_text(prompt=prompt, llm_provider=PROVIDER, llm_model=MODEL)\n",
    "        return self.parse_response(res)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_baselines(word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES) -> List[str]:\n",
    "        # find the baselines.csv file in the `input` folder\n",
    "        filepath = os.path.join('input', 'baselines', f'{word}_{epoch}.baseline_1500_sentences.csv')\n",
    "        df = pd.read_csv(filepath)\n",
    "        # return the `sentence` column as a list\n",
    "        print(f\"Word {word}, epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Loaded {len(df)} baseline sentences, sampling {MAX_BASELINES}\")\n",
    "        baselines = df['sentence'].tolist()\n",
    "        if MAX_BASELINES and len(baselines) > MAX_BASELINES:\n",
    "            baselines = random.sample(baselines, MAX_BASELINES)\n",
    "        baselines = [s.replace(TARGET_WORD, TARGET_WORD) for s in baselines]\n",
    "        return baselines\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_sentences(sentences: List['SentenceToModify'], word: TARGET_WORD_CHOICES, epoch: EPOCH_CHOICES):\n",
    "        # Adjust the directory here to include 'test' when getting pilot/test data\n",
    "        output_dir = 'output'\n",
    "        output_file = os.path.join(output_dir, f'{word}_{epoch}.synthetic_sentences.csv')\n",
    "        \n",
    "        # Ensure the directory exists before trying to write to it\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Creating DataFrame and saving to CSV\n",
    "        df = pd.DataFrame([{'baseline': s.text, 'high_intensity': s.increased_variation, 'low_intensity': s.decreased_variation} for s in sentences])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(sentences)} sentences to {output_file}\")\n",
    "\n",
    "baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "sentence = SentenceToModify(text=baselines[0])\n",
    "print(sentence.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if output\\trauma_1970-1974.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1970-1974: Loaded 12 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:22<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12 sentences to output\\trauma_1970-1974.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1970-1974.synthetic_sentences.csv\n",
      "Checking if output\\trauma_1975-1979.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1975-1979: Loaded 11 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:22<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11 sentences to output\\trauma_1975-1979.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1975-1979.synthetic_sentences.csv\n",
      "Checking if output\\trauma_1980-1984.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1980-1984: Loaded 64 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [02:07<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 64 sentences to output\\trauma_1980-1984.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1980-1984.synthetic_sentences.csv\n",
      "Checking if output\\trauma_1985-1989.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1985-1989: Loaded 118 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [04:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 118 sentences to output\\trauma_1985-1989.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1985-1989.synthetic_sentences.csv\n",
      "Checking if output\\trauma_1990-1994.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1990-1994: Loaded 307 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [10:17<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 307 sentences to output\\trauma_1990-1994.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1990-1994.synthetic_sentences.csv\n",
      "Checking if output\\trauma_1995-1999.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 1995-1999: Loaded 518 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518/518 [16:49<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 518 sentences to output\\trauma_1995-1999.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_1995-1999.synthetic_sentences.csv\n",
      "Checking if output\\trauma_2000-2004.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 2000-2004: Loaded 526 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [17:48<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 526 sentences to output\\trauma_2000-2004.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_2000-2004.synthetic_sentences.csv\n",
      "Checking if output\\trauma_2005-2009.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 2005-2009: Loaded 888 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 888/888 [30:18<00:00,  2.05s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 888 sentences to output\\trauma_2005-2009.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_2005-2009.synthetic_sentences.csv\n",
      "Checking if output\\trauma_2010-2014.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 2010-2014: Loaded 735 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [26:32<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 735 sentences to output\\trauma_2010-2014.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_2010-2014.synthetic_sentences.csv\n",
      "Checking if output\\trauma_2015-2019.synthetic_sentences.csv exists...\n",
      "Word trauma, epoch 2015-2019: Loaded 833 baseline sentences, sampling 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 833/833 [27:57<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 833 sentences to output\\trauma_2015-2019.synthetic_sentences.csv\n",
      "Processed and saved: output\\trauma_2015-2019.synthetic_sentences.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants for processing\n",
    "MAX_BASELINES = 1500\n",
    "OUTPUT_DIR = \"output/5-year\"  # Directory where processed files are saved\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through each target word and epoch\n",
    "for TARGET_WORD in get_args(TARGET_WORD_CHOICES):\n",
    "    for EPOCH in EPOCH_CHOICES:\n",
    "        # Construct the file path to check if it already exists\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{TARGET_WORD}_{EPOCH}.synthetic_sentences.csv\")\n",
    "\n",
    "        # Print the file path to debug\n",
    "        print(f\"Checking if {output_file} exists...\")\n",
    "\n",
    "        # Skip if the file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {TARGET_WORD}, {EPOCH}: File already processed.\")\n",
    "            continue\n",
    "\n",
    "        # Load baselines if the file does not exist\n",
    "        baselines = SentenceToModify.load_baselines(TARGET_WORD, EPOCH)\n",
    "        sentences = []\n",
    "\n",
    "        # Process each baseline sentence\n",
    "        for baseline in tqdm(baselines, total=len(baselines), unit='it', leave=True):\n",
    "            sentence = SentenceToModify(text=baseline)\n",
    "            try:\n",
    "                more_intense_variation, less_intense_variation = sentence.get_variations()\n",
    "                sentences.append(sentence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sentence: {baseline}. Error: {str(e)}\")\n",
    "\n",
    "        # Save the processed sentences\n",
    "        if sentences:  # Only save if there are completed sentences\n",
    "            SentenceToModify.save_sentences(sentences, word=TARGET_WORD, epoch=EPOCH)\n",
    "            print(f\"Processed and saved: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No valid sentences processed for {TARGET_WORD}, {EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation issues found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory containing the synthetic sentence files\n",
    "input_directory = \"output/5-year\"\n",
    "output_directory = \"output/validation_issues\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to validate that all rows in each column contain the target term\n",
    "def validate_target_in_files(directory):\n",
    "    issues_summary = []  # To store summary of issues\n",
    "    all_problematic_rows_combined = pd.DataFrame()  # To store all unique problematic rows across files\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            # Extract the target term from the filename\n",
    "            target_term = file_name.split(\"_\")[0]\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Check if the required columns exist\n",
    "                if {'baseline', 'high_intensity', 'low_intensity'}.issubset(df.columns):\n",
    "                    all_problematic_rows = pd.DataFrame()\n",
    "\n",
    "                    # Use a set to track rows we've already added to avoid duplicates\n",
    "                    seen_rows = set()\n",
    "\n",
    "                    for column in ['baseline', 'high_intensity', 'low_intensity']:\n",
    "                        # Identify rows that do not contain the target term\n",
    "                        problematic_rows = df[~df[column].str.contains(target_term, case=False, na=False)].copy()\n",
    "\n",
    "                        if not problematic_rows.empty:\n",
    "                            # Add the target term, epoch, and row number for identification\n",
    "                            problematic_rows['target'] = target_term  # Add the target term\n",
    "                            problematic_rows['epoch'] = file_name.split(\"_\")[1].split(\".\")[0]  # Extract epoch from filename\n",
    "                            problematic_rows['row_number'] = problematic_rows.index\n",
    "                            problematic_rows['problem_column'] = column\n",
    "\n",
    "                            # Only add rows that have not been added before (check via 'row_number')\n",
    "                            problematic_rows = problematic_rows[~problematic_rows['row_number'].isin(seen_rows)]\n",
    "                            seen_rows.update(problematic_rows['row_number'])\n",
    "\n",
    "                            # Append to the combined DataFrame for this file\n",
    "                            all_problematic_rows = pd.concat([all_problematic_rows, problematic_rows])\n",
    "\n",
    "                    # If there are problematic rows, save them to a new file\n",
    "                    if not all_problematic_rows.empty:\n",
    "                        # Combine with the master list of all rows across all files\n",
    "                        all_problematic_rows_combined = pd.concat([all_problematic_rows_combined, all_problematic_rows])\n",
    "\n",
    "                        issues_summary.append((file_name, len(all_problematic_rows)))\n",
    "\n",
    "                else:\n",
    "                    print(f\"File {file_name} is missing required columns.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    # Remove duplicates across the entire combined DataFrame\n",
    "    if not all_problematic_rows_combined.empty:\n",
    "        all_problematic_rows_combined = all_problematic_rows_combined.drop_duplicates(\n",
    "            subset=['baseline', 'high_intensity', 'low_intensity', 'row_number']\n",
    "        )\n",
    "\n",
    "        # Save the unique problematic rows to a new file\n",
    "        output_file = os.path.join(output_directory, f\"validation_issues_TEST.csv\")\n",
    "        all_problematic_rows_combined.to_csv(output_file, index=False)\n",
    "\n",
    "        # Print summary of issues\n",
    "        print(\"The following files have issues:\")\n",
    "        for file, count in issues_summary:\n",
    "            print(f\"File: {file}, Number of problematic rows: {count}\")\n",
    "        \n",
    "        # Display problematic rows in the notebook\n",
    "        print(\"\\nProblematic rows across all files:\")\n",
    "        display(all_problematic_rows_combined)  # This will display the dataframe in the notebook\n",
    "\n",
    "    else:\n",
    "        print(\"No validation issues found.\")\n",
    "\n",
    "# Run the validation\n",
    "validate_target_in_files(input_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording OpenAI API Credits Usage. Generating more/less variations for: \n",
    "- \"abuse\" = 621.84 - 601.31 = 21 USD\n",
    "- \"anxiety\" = 601.31 - 568.99 = 32 USD\n",
    "- \"depression\" = 568.99 - 534.28 = 35 USD\n",
    "- \"mental_health\" = 534.28 - 510.57 = 24 USD\n",
    "- \"mental_illness\" = 510.57 - 501.11 = 10 USD\n",
    "- \"trauma\" = 501.11 - 487.19 = 14 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and added file: abuse_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: abuse_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: abuse_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: abuse_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: abuse_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: abuse_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: abuse_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: abuse_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: abuse_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: abuse_2015-2019.synthetic_sentences.csv\n",
      "Files for 'abuse' merged successfully into 'output/all-year/abuse_synthetic_sentences.csv'\n",
      "Found and added file: anxiety_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: anxiety_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: anxiety_2015-2019.synthetic_sentences.csv\n",
      "Files for 'anxiety' merged successfully into 'output/all-year/anxiety_synthetic_sentences.csv'\n",
      "Found and added file: depression_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: depression_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: depression_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: depression_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: depression_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: depression_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: depression_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: depression_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: depression_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: depression_2015-2019.synthetic_sentences.csv\n",
      "Files for 'depression' merged successfully into 'output/all-year/depression_synthetic_sentences.csv'\n",
      "Found and added file: mental_health_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: mental_health_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: mental_health_2015-2019.synthetic_sentences.csv\n",
      "Files for 'mental_health' merged successfully into 'output/all-year/mental_health_synthetic_sentences.csv'\n",
      "Found and added file: mental_illness_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: mental_illness_2015-2019.synthetic_sentences.csv\n",
      "Files for 'mental_illness' merged successfully into 'output/all-year/mental_illness_synthetic_sentences.csv'\n",
      "Found and added file: trauma_1970-1974.synthetic_sentences.csv\n",
      "Found and added file: trauma_1975-1979.synthetic_sentences.csv\n",
      "Found and added file: trauma_1980-1984.synthetic_sentences.csv\n",
      "Found and added file: trauma_1985-1989.synthetic_sentences.csv\n",
      "Found and added file: trauma_1990-1994.synthetic_sentences.csv\n",
      "Found and added file: trauma_1995-1999.synthetic_sentences.csv\n",
      "Found and added file: trauma_2000-2004.synthetic_sentences.csv\n",
      "Found and added file: trauma_2005-2009.synthetic_sentences.csv\n",
      "Found and added file: trauma_2010-2014.synthetic_sentences.csv\n",
      "Found and added file: trauma_2015-2019.synthetic_sentences.csv\n",
      "Files for 'trauma' merged successfully into 'output/all-year/trauma_synthetic_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory where the CSV files are located\n",
    "input_directory = 'output/5-year'  # Folder with the 5-year CSV files\n",
    "output_directory = 'output/all-year'  # Folder to save the merged files\n",
    "\n",
    "# List of targets (you can add more targets to this list)\n",
    "targets = ['abuse', 'anxiety', 'depression', 'mental_health', 'mental_illness', 'trauma']\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Loop through each target and process the corresponding files\n",
    "for target in targets:\n",
    "    # Initialize an empty list to collect DataFrames for the current target\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.startswith(target) and file_name.endswith(\"synthetic_sentences.csv\"):  # Match files with the target\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_list.append(df)\n",
    "            print(f\"Found and added file: {file_name}\")  # Debugging print\n",
    "\n",
    "    # Check if any files were added to df_list\n",
    "    if not df_list:\n",
    "        print(f\"No files found for target: {target}\")  # Debugging print\n",
    "\n",
    "    # Concatenate all DataFrames for the current target into one\n",
    "    if df_list:\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Save the merged DataFrame to the new output folder\n",
    "        merged_df.to_csv(os.path.join(output_directory, f'{target}_synthetic_sentences.csv'), index=False)\n",
    "        print(f\"Files for '{target}' merged successfully into 'output/all-year/{target}_synthetic_sentences.csv'\")\n",
    "    else:\n",
    "        print(f\"Skipping '{target}' as no files were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
